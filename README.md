# Generative AI with LangChain & Hugging Face  
Exercises, Projects and Learning Journey

This repository contains all the exercises, experiments, and projects developed throughout my training in **Generative AI using LangChain and Hugging Face**.  
Here I document the concepts I studied, the tools I implemented, and the practical skills I applied while building real generative AI systems from scratch.

---

## ðŸš€ What I Learned

### ðŸ”¹ Understanding Generative AI in Depth
I explored how generative models work, why they differ from traditional ML models, and how they are used to create text, automate tasks, generate content, and power modern AI applications.  
This included learning about model architectures, tokenization, embeddings, and the workflows behind LLM-powered systems.

---

### ðŸ”¹ LangChain Fundamentals
I learned how to use **LangChain** as the framework that connects different AI components into structured applications.  
This involved understanding and working with:

- LLM chains  
- Prompt templates  
- Tools & tool calling  
- Agents and agent executors  
- Memory modules  
- Document loaders and vector stores  
- End-to-end pipelines for AI apps  

Throughout the course, I built multiple chains and tested how each component affects the behavior of the AI system.

---

### ðŸ”¹ Hugging Face Model Integration
I integrated Hugging Face models directly into LangChain applications, learning how to:

- Load pre-trained transformer models  
- Use pipelines for text generation and embeddings  
- Customize and fine-tune models for specific tasks  
- Optimize model performance and latency  
- Combine Hugging Face with LangChain abstractions  

This gave me practical experience in mixing modern open-source NLP models with custom architectures.

---

### ðŸ”¹ Building Full Generative AI Applications
Throughout the course I created practical generative AI applications, including:

- Chatbots using custom prompts, chains, and memory  
- Automated content generators  
- Tools for text transformation and data augmentation  
- Structured output generation  
- Assistants capable of using tools, retrieving information and reasoning  

Every application was built step-by-step, applying concepts from previous modules.

---

### ðŸ”¹ RAG (Retrieval-Augmented Generation)
One of the most important skills acquired was building **RAG pipelines**, which combine retrieval systems with generative models for higher accuracy.

I learned how to:

- Process and chunk documents  
- Generate embeddings  
- Store data in vector databases  
- Retrieve context based on similarity  
- Integrate the retrieved context into LLM responses  

This resulted in AI systems capable of answering questions based on private knowledge sources.

---

### ðŸ”¹ Deployment Strategies
I studied different approaches to deploying AI systems, including:

- Cloud deployment (API-based or containerized)  
- On-premise hosting setups  
- Scaling models for production workloads  
- Environment management, versioning and CI/CD concepts  
- Exposing models through APIs and serverless functions  

This gave me a practical view of how real AI applications go live and stay reliable.

---

### ðŸ”¹ Monitoring, Optimization & Maintenance
I learned techniques to keep deployed models running efficiently:

- Logging and tracking model behavior  
- Performance and latency optimization  
- Updating models without breaking pipelines  
- Improving prompts and retrieval quality  
- Best practices for long-term AI lifecycle management  

These skills are essential for maintaining production-ready generative systems.